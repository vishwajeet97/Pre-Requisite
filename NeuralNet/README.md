# Basics Of Neural Net

## Non-linear functions
* [Explanation of non-linear transformation](https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/)
* [Demo](http://cs.stanford.edu/people/karpathy/convnetjs//demo/classify2d.html)
* [Why use non-linear activation?](https://www.quora.com/Why-does-deep-learning-architectures-use-only-non-linear-activation-function-in-the-hidden-layers)

## Universal Approximation Theorem
* [Explanation with interactive visualization](http://neuralnetworksanddeeplearning.com/chap4.html) - *Michael Nielsen*
* [Fun post](https://medium.com/towards-data-science/can-neural-networks-really-learn-any-function-65e106617fc6) - *Brendan Fortuner*

## Convex Optimization
* [Wikipedia link](https://en.wikipedia.org/wiki/Convex_optimization)
* [CS229 handouts - Part1](https://see.stanford.edu/materials/aimlcs229/cs229-cvxopt.pdf)
* [CS229 handouts - Part2](http://cs229.stanford.edu/section/cs229-cvxopt2.pdf)
* [Gradient Descent applied to Linear Regression](https://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/)
* [Lecture Note 1 of CS229](http://cs229.stanford.edu/notes/cs229-notes1.pdf)

## Backpropogation
* [Explanaton of the Algorithm](http://neuralnetworksanddeeplearning.com/chap2.html)
* [Coding up backpropogation](http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/)

## Problems with adding more layers

### Vanishing Gradient Problem
* [Wikipedia link](https://en.wikipedia.org/wiki/Vanishing_gradient_problem)
* [Unstable Gradient Problem](http://neuralnetworksanddeeplearning.com/chap5.html)
* [Blog post by Rohan Kapur](https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b)

### Local Minima
Not much of a problem now as a local minima is equally good enough
* [all local minimum have very similar function value to the global optimum](https://stats.stackexchange.com/questions/203288/understanding-almost-all-local-minimum-have-very-similar-function-value-to-the)

## Loss functions
* [Wikipedia link](https://en.wikipedia.org/wiki/Loss_functions_for_classification)
* [Wikipedia link for Cross Entropy](https://en.wikipedia.org/wiki/Cross_entropy)
* [Wikipedia link for Hinge Loss](https://en.wikipedia.org/wiki/Hinge_loss)

## Adaptive Learning Rate
* [Why momentum really works](http://distill.pub/2017/momentum/) - *GABRIEL GOH, UC Davis*
* [Optimizing Gradient Descent](http://sebastianruder.com/optimizing-gradient-descent/) - *Sebastien Ruder*

